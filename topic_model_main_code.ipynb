{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification by using Topic model (LDA)  \n",
    "2019/01/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Impact    \n",
    "　Ted Talk や Medium などのテキスト、音声、動画等全てのコンテンツプラットフォームにおいてトピックカテゴリごとにユーザーからパブリッシュされたコンテンツを自動分類する役割を担っている。    \n",
    "　このトピックごとの分類をもとに適切なレコメンドが可能となる。確率分布、共役事前分布、パラメータ推定の３点がかなり重要になる。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://theintelligenceofinformation.files.wordpress.com/2016/12/topic-modeling-for-learning-analytics-researchers-lak15-tutorial-15-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【MY GOAL】\n",
    "### トピックモデルを使って、ニュースタイトルを解析してカテゴリー分類する機械学習モデルを構築する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.【学習/テストデータの準備】 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"abcnews-date-text.csv\", error_bad_lines=False)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "記事数 : 1103663\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = data[[\"headline_text\"]]\n",
    "title[\"index\"] = title.index\n",
    "print(\"記事数 : {}\".format(len(title)))\n",
    "title[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 【自然言語の前処理】Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは以下、5つの処理を行う。 \n",
    "   \n",
    "1. Tokenization 記事を文単位に。文章を単語単位に分解する作業。   \n",
    "2. Removing meaningless words 3アルファベット以下の単語の除外（英語の自然言語処理でよく用いられる前処理）。  \n",
    "3. Stopwords ある言語全体で頻出するため特徴を表現するものではない単語の除外。  \n",
    "4. Lemmatizing 時制や形式の統一（meeting, meets, met→ meet）。   \n",
    "5. Stemming 同じ語幹を持つ単語の統一。   \n",
    "##### gensim(1&3)とnltk(4&5)というNLPの前処理のためのライブラリをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/akr712/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import * \n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "# corpusをダウンロード\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizingの例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"went\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmingの例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed word\n",
       "0       caresses       caress\n",
       "1          flies          fli\n",
       "2           dies          die\n",
       "3          mules         mule\n",
       "4         denied         deni\n",
       "5           died          die\n",
       "6         agreed         agre\n",
       "7          owned          own\n",
       "8        humbled        humbl\n",
       "9          sized         size\n",
       "10       meeting         meet\n",
       "11       stating        state\n",
       "12       siezing         siez\n",
       "13   itemization         item\n",
       "14   sensational       sensat"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational']\n",
    "\n",
    "stemmed_words = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed word': stemmed_words})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1~5で一気に前処理（パイプライン処理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元データのタイトルテキストと、前処理後のタイトルを比較する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing : ['hunter', 'police', 'prepare', 'for', 'potential', 'terrorism']\n",
      "After preprocessing : ['hunter', 'polic', 'prepar', 'potenti', 'terror']\n"
     ]
    }
   ],
   "source": [
    "tidy_text = title[title[\"index\"]==2019].values[0][0]\n",
    "\n",
    "word_list = []\n",
    "for word in tidy_text.split(\" \"):\n",
    "    word_list.append(word)\n",
    "print(\"Before preprocessing : {}\".format(word_list))\n",
    "print(\"After preprocessing : {}\".format(preprocess(tidy_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidy_titles = title[\"headline_text\"].map(preprocess)\n",
    "tidy_titles[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 【自然言語の数値化】Bag of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRecLU_sSXnDqdNI_i7xSRMWKys5VSTUoVDMDPyfWurcZZi_CR3WA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからオリジナルのコーパスを作成し、ベクトル化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary \n",
    "dictionary = Dictionary(tidy_titles)\n",
    "\n",
    "count = 0\n",
    "for k, i in dictionary.iteritems():\n",
    "    print(k, i)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・ \"no_below=15\" 15回以下しか出現しない単語は無視します   \n",
    "・ \"no_above=0.5\" 全部の文章の50パーセント以上に出現したワードは一般的すぎるワードとして無視します   \n",
    "・ \"keep_n=100000\" 10万語までがコーパスに載る限界単語数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オリジナルコーパスに載せた前処理済みの単語を、ベクトル化（＝数値化）する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(586, 1), (643, 1), (4235, 1), (11110, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(word) for word in tidy_titles]\n",
    "bow_corpus[99999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オリジナルコーパスに載せた単語と情報を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 50 (\"hop\") appears 1 time.\n",
      "word 85 (\"defeat\") appears 1 time.\n",
      "word 382 (\"zealand\") appears 1 time.\n",
      "word 2960 (\"lanka\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_title_5000 = bow_corpus[5000]\n",
    "\n",
    "for i in range(len(bow_title_5000)):\n",
    "    print(\"word {} (\\\"{}\\\") appears {} time.\".format(bow_title_5000[i][0], \n",
    "                                                     dictionary[bow_title_5000[i][0]], \n",
    "                                                     bow_title_5000[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【重要単語評価】Remove More Words from Corpus by uisng TF-IDF  \n",
    "TF-IDFを使ってタイトル内における単語の重要度を評価し、ベクトル化までする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5892908867507543),\n",
      " (1, 0.38929654337861147),\n",
      " (2, 0.4964985175717023),\n",
      " (3, 0.5046520327464028)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for word in corpus_tfidf:\n",
    "    pprint(word)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.【文書分類：トピックモデル】Topic Modeling with LSA, PLSA, LDA & lda2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/1200/1*_ZMgTsJGmR743ngZ7UxN9w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) LDA : Latent Dirichlet Allocation   \n",
    "LDAは文章中の潜在的なトピックを推定し、文章分類や、文章ベクトルの次元削減等に用いられる技術です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-1) LDA with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.028*\"elect\" + 0.019*\"say\" + 0.018*\"death\" + 0.016*\"hospit\" + 0.016*\"tasmanian\" + 0.014*\"labor\" + 0.013*\"deal\" + 0.012*\"china\" + 0.011*\"polit\" + 0.011*\"talk\"\n",
      "Topic: 1 \n",
      "Words: 0.020*\"nation\" + 0.018*\"coast\" + 0.017*\"help\" + 0.016*\"countri\" + 0.015*\"state\" + 0.015*\"chang\" + 0.015*\"health\" + 0.013*\"hour\" + 0.013*\"indigen\" + 0.012*\"water\"\n",
      "Topic: 2 \n",
      "Words: 0.020*\"canberra\" + 0.018*\"market\" + 0.014*\"rise\" + 0.014*\"west\" + 0.014*\"australian\" + 0.014*\"turnbul\" + 0.013*\"price\" + 0.013*\"share\" + 0.012*\"victoria\" + 0.011*\"bank\"\n",
      "Topic: 3 \n",
      "Words: 0.063*\"polic\" + 0.023*\"crash\" + 0.019*\"interview\" + 0.019*\"miss\" + 0.018*\"shoot\" + 0.016*\"arrest\" + 0.015*\"investig\" + 0.013*\"driver\" + 0.012*\"search\" + 0.011*\"offic\"\n",
      "Topic: 4 \n",
      "Words: 0.029*\"charg\" + 0.028*\"court\" + 0.021*\"murder\" + 0.018*\"woman\" + 0.018*\"face\" + 0.016*\"alleg\" + 0.016*\"brisban\" + 0.015*\"live\" + 0.015*\"jail\" + 0.014*\"accus\"\n",
      "Topic: 5 \n",
      "Words: 0.035*\"australia\" + 0.021*\"melbourn\" + 0.021*\"world\" + 0.016*\"open\" + 0.015*\"sydney\" + 0.014*\"final\" + 0.013*\"donald\" + 0.010*\"leagu\" + 0.010*\"take\" + 0.010*\"win\"\n",
      "Topic: 6 \n",
      "Words: 0.026*\"south\" + 0.025*\"kill\" + 0.015*\"island\" + 0.013*\"fall\" + 0.011*\"attack\" + 0.010*\"forc\" + 0.009*\"shark\" + 0.008*\"east\" + 0.007*\"christma\" + 0.007*\"northern\"\n",
      "Topic: 7 \n",
      "Words: 0.015*\"power\" + 0.013*\"farmer\" + 0.013*\"council\" + 0.011*\"guilti\" + 0.010*\"region\" + 0.010*\"feder\" + 0.010*\"research\" + 0.009*\"industri\" + 0.009*\"futur\" + 0.009*\"energi\"\n",
      "Topic: 8 \n",
      "Words: 0.036*\"trump\" + 0.032*\"australian\" + 0.019*\"queensland\" + 0.014*\"leav\" + 0.014*\"australia\" + 0.011*\"show\" + 0.011*\"report\" + 0.011*\"royal\" + 0.011*\"say\" + 0.010*\"game\"\n",
      "Topic: 9 \n",
      "Words: 0.036*\"govern\" + 0.020*\"test\" + 0.018*\"rural\" + 0.014*\"break\" + 0.013*\"school\" + 0.012*\"news\" + 0.010*\"violenc\" + 0.010*\"say\" + 0.010*\"worker\" + 0.009*\"premier\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_bow.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-2) LDA with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) LSI : Latent Semantic Indexing    \n",
    "LSIは、日本語では潜在的意味解析と呼ばれ、非常に端的にこの技術をまとめると、「自動車」「車」等、同じ意味をもつ単語をうまくグルーピングして、文章中の情報量を凝縮して、要点を強めようという圧縮技術です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.【モデルの比較・評価・改善】Validation for models and Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_titles[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_bow[tidy_titles[3000]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_bow.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = \"How Artificial Interigence change plitics in United States\"\n",
    "bow_vector = dictionary.doc2bow(preprocess(test_title))\n",
    "\n",
    "for index, score in sorted(lda_bow[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_bow.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.tidytextmining.com/images/tidyflow-ch-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・尤度：観察データの下での仮説のもっともらしさ   \n",
    "・最尤推定：事前確率を使用せずに尤度のみでパラメータを推定   \n",
    "・ベイズ推定：今取得したデータだけでなく、過去の推定結果や経験に基づく事前確率も使用してパラメータを推定   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
